# -*- coding: utf-8 -*-
"""Qdrant,Milvus,pgvector.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1U1pomYM9VvNhVrcd00dF0XFPk0Oh6GQS

Load Pdf and split into chunks
"""

!pip install langchain_community pymupdf
from transformers import AutoTokenizer, AutoModel, AutoModelForSeq2SeqLM, pipeline
from langchain_community.document_loaders import PyMuPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
import torch
import numpy as np

pdf_path = "APJAbdulKalam.pdf"
loader = PyMuPDFLoader(pdf_path)
docs = loader.load()

splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)
split_docs = splitter.split_documents(docs)
documents = [doc.page_content for doc in split_docs]
metadatas = [{"source": doc.metadata.get("page", "unknown")} for doc in split_docs]

"""Embedding Model - Jina AI"""

embedding_model_name = "jinaai/jina-embeddings-v2-base-code"
embedding_tokenizer = AutoTokenizer.from_pretrained(embedding_model_name, trust_remote_code=True)
embedding_model = AutoModel.from_pretrained(embedding_model_name, trust_remote_code=True)

def get_embedding(texts):
    inputs = embedding_tokenizer(texts, padding=True, truncation=True, return_tensors="pt")
    with torch.no_grad():
        outputs = embedding_model(**inputs)
        embeddings = outputs.last_hidden_state.mean(dim=1)
    return embeddings.cpu().numpy()

embeddings = get_embedding(documents)
embeddings

"""Qdrant Database"""

!pip install qdrant-client

"""sudo docker pull qdrant/qdrant

sudo docker run -p 6333:6333 -p 6334:6334 \
    -v "$(pwd)/qdrant_storage:/qdrant/storage:z" \
    qdrant/qdrant
"""

from qdrant_client import QdrantClient
from qdrant_client.http.models import Distance, VectorParams

# Connect to local Qdrant
client = QdrantClient(host="localhost", port=6333)

client.recreate_collection(
    collection_name="kalam_collection",
    vectors_config=VectorParams(
        size=768,                    # Use correct vector size here
        distance=Distance.COSINE     # Or Distance.DOT / Distance.EUCLID
    )
)

ids = list(range(len(embeddings)))
vectors = [vec.tolist() for vec in embeddings]
payloads = [{"text": doc} for doc in documents]

client.upsert(
    collection_name="kalam_collection",
    wait=True,
    points=[
        {
            "id": id_,
            "vector": vector,
            "payload": payload
        }
        for id_, vector, payload in zip(ids, vectors, payloads)
    ]
)

"""Query"""

query_text = "Who is Abdul Kalam?"
query_embedding = get_embedding([query_text])[0].tolist()  # your embedding function

"""Qdrant Results"""

qdrant_search_results = client.search(
    collection_name="kalam_collection",
    query_vector=query_embedding,
    limit=3,            # top 3 results
    with_payload=True   # to get stored text with results
)
qdrant_relevant_docs = [hit.payload['text'] for hit in qdrant_search_results]

for hit in qdrant_search_results:
    print(f"Score: {hit.score:.4f}")
    print(f"Text: {hit.payload['text']}")
    print("-----")

"""LLM"""

llm_model_name = "google/flan-t5-base"
llm_tokenizer = AutoTokenizer.from_pretrained(llm_model_name)
llm_model = AutoModelForSeq2SeqLM.from_pretrained(llm_model_name)

llm_pipe = pipeline(
    "text2text-generation",
    model=llm_model,
    tokenizer=llm_tokenizer,
    max_length=512,
    device=0 if torch.cuda.is_available() else -1
)

qdrant_context = "\n".join(qdrant_relevant_docs)

print(f"\nüß† Question: {query_text}")

final_input = f"""You are an expert assistant.
Answer the following question strictly only based on the provided context."

Context:
{qdrant_context}

Question: {query_text}
Answer:"""

response = llm_pipe(final_input)[0]["generated_text"]
print("\n‚úÖQdrant Answer:\n", response)

"""Milvus"""

!pip uninstall grpcio grpcio-status pymilvus -y
!pip install grpcio==1.67.1 grpcio-status==1.67.1
!pip install pymilvus==2.5.0

"""wget https://github.com/milvus-io/milvus/releases/download/v2.5.13/milvus-standalone-docker-compose.yml -O docker-compose.yml

sudo docker compose up -d

sudo docker compose down

"""

from pymilvus import connections, Collection, CollectionSchema, FieldSchema, DataType, utility

connections.connect("default", host="localhost", port="19530")

collection_name = "pdf_rag"

if utility.has_collection(collection_name):
    utility.drop_collection(collection_name)

fields = [
    FieldSchema(name="id", dtype=DataType.INT64, is_primary=True, auto_id=True),
    FieldSchema(name="content", dtype=DataType.VARCHAR, max_length=1000),
    FieldSchema(name="embedding", dtype=DataType.FLOAT_VECTOR, dim=768)
]

schema = CollectionSchema(fields)
collection = Collection(name=collection_name, schema=schema)

collection.create_index(field_name="embedding", index_params={
    "metric_type": "COSINE", "index_type": "FLAT", "params": {}
})

collection.load()

import time
def safe_insert_data(collection, documents, embeddings, batch_size=20):
    """Insert data in batches with proper error handling"""

    total_docs = len(documents)
    print(f"Starting insertion of {total_docs} documents in batches of {batch_size}")

    for i in range(0, total_docs, batch_size):
        try:
            # Get batch data
            end_idx = min(i + batch_size, total_docs)
            batch_docs = documents[i:end_idx]
            batch_embs = embeddings[i:end_idx].tolist()

            # CRITICAL: Check document length and truncate if necessary
            processed_docs = []
            for doc in batch_docs:
                if len(doc) > 2000:  # Max length we set in schema
                    processed_docs.append(doc[:1990] + "...")  # Truncate with ellipsis
                else:
                    processed_docs.append(doc)

            # Insert batch - CORRECT FORMAT: [field1_data, field2_data]
            # Note: Don't include 'id' field data since auto_id=True
            insert_result = collection.insert([processed_docs, batch_embs])

            # Flush to ensure data is written
            collection.flush()

            print(f"‚úÖ Inserted batch {i+1}-{end_idx}: {len(insert_result.primary_keys)} records")

            # Small delay to prevent overwhelming the system
            time.sleep(0.5)

        except Exception as e:
            print(f"‚ùå Failed to insert batch {i+1}-{end_idx}: {e}")
            print(f"Error type: {type(e).__name__}")

            # Try to diagnose the issue
            if "string length" in str(e).lower():
                print("üí° Issue: Document too long. Increase max_length in schema or truncate documents.")
            elif "dimension" in str(e).lower():
                print("üí° Issue: Embedding dimension mismatch. Check your embedding model output.")
            elif "connection" in str(e).lower():
                print("üí° Issue: Connection problem. Check if Milvus is running.")

            return False
    collection.flush()
    print(f"‚úÖ Total records in collection: {collection.num_entities}")
    return True

# Perform the insertion
success = safe_insert_data(collection, documents, embeddings, batch_size=20)

if success:
    print("üéâ All data inserted successfully!")

"""Milvus Results"""

search_params = {"metric_type": "COSINE","params": {}}

results = collection.search(data=[query_embedding],anns_field="embedding",param=search_params,limit=3,output_fields=["content"])

print(f"\nüîç Search results for: '{query_text}'")

milvus_relavant_docs=[]
for i, hit in enumerate(results[0]):
    print(f"Result {i+1}:")
    print(f"Score: {hit.score:.4f}")
    print(f"Content: {hit.entity.get('content')[:]}")
    milvus_relavant_docs.append(hit.entity.get('content')[:])
    print()

milvus_context = "\n".join(milvus_relavant_docs)

print(f"\nüß† Question: {query_text}")

final_input = f"""You are an expert assistant.
Answer the following question strictly only based on the provided context."

Context:
{milvus_context}

Question: {query_text}
Answer:"""

milvus_response = llm_pipe(final_input)[0]["generated_text"]
print("\n‚úÖMilvus Answer:\n", milvus_response)

import psycopg
from pgvector.psycopg import register_vector

# Connect to DB and register pgvector
conn = psycopg.connect(
    host='localhost',
    port='5432',
    dbname='pgvector',
    user='postgres',
    password='R.Karthik@04',
    autocommit=True
)

conn.execute('CREATE EXTENSION IF NOT EXISTS vector')
register_vector(conn)

# Drop and create table
conn.execute('DROP TABLE IF EXISTS rag_chunks')
conn.execute('CREATE TABLE rag_chunks (id bigserial PRIMARY KEY, content text, embedding vector(768), metadata text)')

# Store data
cur = conn.cursor()
with cur.copy('COPY rag_chunks (content, embedding, metadata) FROM STDIN WITH (FORMAT BINARY)') as copy:
    copy.set_types(['text', 'vector', 'text'])
    for content, embedding, meta in zip(documents, embeddings, metadatas):
        copy.write_row([content, embedding, str(meta)])

def retrieve_context(query: str, top_k=3):
    query_embedding1 = get_embedding([query])[0]  # shape (768,)

    # Fetch content along with similarity score
    result = conn.execute(
        '''
        SELECT content, embedding <=> %s AS score
        FROM rag_chunks
        ORDER BY score
        LIMIT %s
        ''',
        (np.array(query_embedding1), top_k)
    ).fetchall()

    print(f"\nTop {top_k} chunks for query: '{query}'\n" + "-"*60)
    for i, (content, score) in enumerate(result, start=1):
        print(f"\nChunk {i} (Score: {score:.4f}):\n{content}\n" + "-"*60)

    context = '\n\n'.join([row[0] for row in result])
    return context

pgvector_context=retrieve_context(query_text)

print(f"\nüß† Question: {query_text}")

final_input = f"""You are an expert assistant.
Answer the following question strictly only based on the provided context."

Context:
{pgvector_context}

Question: {query_text}
Answer:"""

pgvector_response = llm_pipe(final_input)[0]["generated_text"]
print("\n‚úÖPgvector Answer:\n", milvus_response)

